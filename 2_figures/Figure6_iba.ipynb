{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0383a648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T14:29:30.017224Z",
     "iopub.status.busy": "2022-08-30T14:29:30.017037Z",
     "iopub.status.idle": "2022-08-30T14:29:37.902328Z",
     "shell.execute_reply": "2022-08-30T14:29:37.901364Z",
     "shell.execute_reply.started": "2022-08-30T14:29:30.017175Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'captum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3640961/4074280566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcaptum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcaptum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntegratedGradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoiseTunnel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputXGradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'captum'"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, NoiseTunnel, InputXGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f770698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinalrisk.training import setup_training\n",
    "from retinalrisk.models.supervised import SupervisedTraining\n",
    "\n",
    "from IBA.pytorch import IBA, tensor_to_np_img, get_imagenet_folder, imagenet_transform\n",
    "from IBA.utils import plot_saliency_map, to_unit_interval, load_monkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_PATH = '/sc-projects/sc-proj-ukb-cvd/data/2_datasets_pre/211110_anewbeginning/artifacts/baseline_outcomes_220627.feather'\n",
    "MODEL_PATH = '/sc-projects/sc-proj-ukb-cvd/results/models/retina/rids0apm/epoch=127-step=47744.ckpt'\n",
    "BASE_DIR = '/sc-projects/sc-proj-ukb-cvd/results/projects/22_retina_phewas_220812'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ed9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch_geometric import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19034293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "from random import choice\n",
    "from omegaconf import ListConfig\n",
    "\n",
    "\n",
    "class AdaptiveRandomCropTransform(nn.Module):\n",
    "    def __init__(\n",
    "        self, crop_ratio: Union[list, float], out_size: int, interpolation=PIL.Image.BILINEAR\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.crop_ratio = crop_ratio\n",
    "        self.out_size = out_size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, sample):\n",
    "        input_size = min(sample.size)\n",
    "        if isinstance(self.crop_ratio, (list, ListConfig)):\n",
    "            crop_ratio = choice(self.crop_ratio)\n",
    "        else:\n",
    "            crop_ratio = self.crop_ratio\n",
    "\n",
    "        crop_size = int(crop_ratio * input_size)\n",
    "        if crop_size < self.out_size:\n",
    "            crop_size = tv.transforms.transforms._setup_size(\n",
    "                self.out_size, error_msg=\"Please provide only two dimensions (h, w) for size.\"\n",
    "            )\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(sample, crop_size)\n",
    "            return TF.crop(sample, i, j, h, w)\n",
    "        else:\n",
    "            crop_size = tv.transforms.transforms._setup_size(\n",
    "                crop_size, error_msg=\"Please provide only two dimensions (h, w) for size.\"\n",
    "            )\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(sample, crop_size)\n",
    "            cropped = TF.crop(sample, i, j, h, w)\n",
    "        out = TF.resize(cropped, self.out_size, self.interpolation)\n",
    "        return out, (i, j, h, w)\n",
    "\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.encoder(x)\n",
    "        return self.head(x_hat)[\"logits\"]\n",
    "\n",
    "\n",
    "def loader_wrapper(loader):\n",
    "    for batch in loader:\n",
    "        yield batch.data, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16f2d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "records = pd.read_feather(\n",
    "    #\"/sc-projects/sc-proj-ukb-cvd/data/2_datasets_pre/211110_anewbeginning/artifacts/final_records_omop_220531.feather\",\n",
    "    ARTIFACT_PATH,\n",
    "    columns=[\"eid\", \"recruitment_date\", \"concept_id\", \"date\"],\n",
    ")\n",
    "records.concept_id = records.concept_id.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = (\n",
    "    #\"/sc-projects/sc-proj-ukb-cvd/results/models/retina/1zkzua6h/checkpoints/last.ckpt\" # fullrun_jun\n",
    "    MODEL_PATH # fullrun_aug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.initialize(config_path=\"dev/projects/RetinalRisk/config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = hydra.compose(\n",
    "    config_name=\"config\",\n",
    "    overrides=[\n",
    "        \"training.gradient_checkpointing=False\",\n",
    "        \"datamodule/covariates=no_covariates\",\n",
    "        \"model=image\",\n",
    "        \"setup.use_data_artifact_if_available=False\",\n",
    "        \"head=mlp\",\n",
    "        \"head.kwargs.num_hidden=512\",\n",
    "        \"head.kwargs.num_layers=2\",\n",
    "        \"head.dropout=0.5\",\n",
    "        \"training.optimizer_kwargs.weight_decay=0.001\",\n",
    "        \"training.optimizer_kwargs.lr=0.0001\",\n",
    "        \"model.freeze_encoder=False\",\n",
    "        \"model.encoder=convnext_small\",\n",
    "        \"datamodule.batch_size=32\",\n",
    "        \"training.warmup_period=8\",\n",
    "        \"datamodule/augmentation=contrast_sharpness_posterize\",\n",
    "        \"datamodule.img_size_to_gpu=420\",\n",
    "        \"datamodule.num_workers=8\",\n",
    "        \"model.pretrained=True\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a45043",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule, model, _ = setup_training(cfg)\n",
    "datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c354502",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_transform = AdaptiveRandomCropTransform(\n",
    "    crop_ratio=datamodule.test_dataset.img_crop_ratio,\n",
    "    out_size=datamodule.test_dataset.img_size_to_gpu,\n",
    "    interpolation=PIL.Image.BICUBIC,\n",
    ")\n",
    "\n",
    "remaining_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc37b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_kwargs = dict(\n",
    "    label_mapping=datamodule.label_mapping,\n",
    ")\n",
    "\n",
    "model = SupervisedTraining.load_from_checkpoint(\n",
    "    checkpoint_path, encoder=model.encoder, head=model.head, **training_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model_wrapped = ModelWrapper(model.encoder, model.head).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81043907",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n",
    "std = np.array([0.229, 0.224, 0.225])[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "iba = IBA(model_wrapped.encoder.features[3])\n",
    "iba.estimate(\n",
    "    model_wrapped, loader_wrapper(datamodule.val_dataloader()), n_samples=10000, progbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = [\n",
    "    # generally very important\n",
    "    \"phecode_202 - Diabetes mellitus\",\n",
    "    \"phecode_404 - Ischemic heart disease\",\n",
    "    \"phecode_404-1 - Myocardial infarction [Heart attack]\",\n",
    "    \"phecode_431-11 - Cerebral infarction [Ischemic stroke]\",\n",
    "    \"phecode_424 - Heart failure\",\n",
    "    \"OMOP_4306655 - All-Cause Death\",\n",
    "    # also generally important and relevant\n",
    "    \"phecode_440-3 - Pulmonary embolism\",\n",
    "    \"phecode_468 - Pneumonia\",\n",
    "    \"phecode_474 - Chronic obstructive pulmonary disease [COPD]\",\n",
    "    \"phecode_542 - Chronic liver disease and sequelae\",\n",
    "    \"phecode_583 - Chronic kidney disease\",\n",
    "    \"phecode_328 - Dementias and cerebral degeneration\",\n",
    "    # generally important and fun to check\n",
    "    \"phecode_164 - Anemia\",\n",
    "    \"phecode_726-1 - Osteoporosis\",\n",
    "    \"phecode_103 - Malignant neoplasm of the skin\",\n",
    "    \"phecode_101 - Malignant neoplasm of the digestive organs\",\n",
    "    \"phecode_665 - Psoriasis\",\n",
    "    \"phecode_705-1 - Rheumatoid arthritis\",\n",
    "    # important for eye\n",
    "    \"phecode_371 - Cataract\",\n",
    "    \"phecode_374-3 - Retinal vascular changes and occlusions\",\n",
    "    \"phecode_374-42 - Diabetic retinopathy\",\n",
    "    \"phecode_374-5 - Macular degeneration\",\n",
    "    \"phecode_375-1 - Glaucoma\",\n",
    "    \"phecode_388 - Blindness and low vision\",\n",
    "    # other eye\n",
    "    \"phecode_374-51 - Age-related macular degeneration\",\n",
    "    \"phecode_367-5 - Uveitis\"\n",
    "]\n",
    "\n",
    "prefix = \"220812\"\n",
    "suffix = \"_features3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c15a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_endpoint = 4\n",
    "samples_per_image = 256\n",
    "\n",
    "for combined_endpoint in endpoints:\n",
    "    endpoint, endpoint_name = combined_endpoint.split(\" - \")\n",
    "    endpoint_idx = np.argwhere([l == endpoint for l in datamodule.labels])[0, 0]\n",
    "\n",
    "    endpoint_records = records[records.concept_id.isin([endpoint.replace(\"-\", \".\")])].reset_index()\n",
    "    # endpoint_records = endpoint_records.query(\"date <= recruitment_date\")\n",
    "\n",
    "    endpoint_records[\"td\"] = (endpoint_records.date - endpoint_records.recruitment_date).abs()\n",
    "    endpoint_records = endpoint_records.sort_values(\"td\").drop_duplicates(\n",
    "        subset=[\"eid\"], keep=\"first\"\n",
    "    )\n",
    "    all_eids = set(endpoint_records.eid.values)\n",
    "    eid_subset = endpoint_records[endpoint_records.td < datetime.timedelta(days=180)].eid.values\n",
    "\n",
    "    test_eid_subset = [e for e in eid_subset if e in datamodule.test_dataset.eids]\n",
    "    sample_idxs = [list(datamodule.test_dataset.eids).index(e) for e in test_eid_subset]\n",
    "\n",
    "    for idx in range(min(num_per_endpoint, len(sample_idxs))):\n",
    "        path = datamodule.test_dataset.retina_map[\"file_path\"].values[sample_idxs[idx]]\n",
    "        img = datamodule.test_dataset.loader(path)\n",
    "\n",
    "        attributions = []\n",
    "        for _ in range(samples_per_image):\n",
    "            img_cropped, img_cropped_coords = crop_transform(img)\n",
    "            img_tensor = remaining_transforms(img_cropped)\n",
    "\n",
    "            crop_h = (img_tensor.shape[1] - 384) // 2\n",
    "            crop_w = (img_tensor.shape[2] - 384) // 2\n",
    "\n",
    "            input = img_tensor[None, :, crop_w:-crop_w, crop_h:-crop_h].clone().detach().to(device)\n",
    "            model_loss_closure = lambda x: -model_wrapped(x)[:, endpoint_idx].mean()\n",
    "            saliency_map = iba.analyze(input, model_loss_closure, beta=10)\n",
    "\n",
    "            saliency_map_full = np.zeros((420, 420), dtype=np.float32) * np.nan\n",
    "            saliency_map_full[crop_w:-crop_w, crop_h:-crop_h] = saliency_map\n",
    "\n",
    "            saliency_map_full_resized = TF.resize(\n",
    "                torch.from_numpy(saliency_map_full).unsqueeze(0),\n",
    "                img_cropped_coords[2:],\n",
    "                PIL.Image.BICUBIC,\n",
    "            ).numpy()[0]\n",
    "\n",
    "            attribution = np.zeros((img.height, img.width), dtype=np.float32) * np.nan\n",
    "            attribution[\n",
    "                img_cropped_coords[0] : img_cropped_coords[0] + img_cropped_coords[2],\n",
    "                img_cropped_coords[1] : img_cropped_coords[1] + img_cropped_coords[3],\n",
    "            ] = saliency_map_full_resized\n",
    "\n",
    "            attributions.append(attribution)\n",
    "\n",
    "        attribution = np.stack(attributions)\n",
    "        attribution = np.nanmean(attribution, axis=0)\n",
    "        attribution[np.isnan(attribution)] = 0\n",
    "\n",
    "        fig, ax = captum.attr.visualization.visualize_image_attr_multiple(\n",
    "            attribution[:, :, None],\n",
    "            np.asarray(img),\n",
    "            [\"original_image\", \"heat_map\", \"blended_heat_map\"],\n",
    "            [\"all\", \"positive\", \"positive\"],\n",
    "            use_pyplot=False,\n",
    "            fig_size=(12, 6),\n",
    "        )\n",
    "        managed_fig = plt.figure()\n",
    "        canvas_manager = managed_fig.canvas.manager\n",
    "        canvas_manager.canvas.figure = fig\n",
    "        fig.set_canvas(canvas_manager.canvas)\n",
    "        plt.suptitle(f\"Attribution for {endpoint_name} (IBA)\", y=0.85, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"{BASE_DIR}/attributions/{prefix}_{datamodule.test_dataset.eids[sample_idxs[idx]]}_{endpoint}_{endpoint_name}_IBA{suffix}.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605dcd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for combined_endpoint in endpoints:\n",
    "    endpoint, endpoint_name = combined_endpoint.split(\" - \")\n",
    "\n",
    "    base_path = pathlib.Path(\n",
    "        #f\"/sc-projects/sc-proj-ukb-cvd/results/projects/22_retina_phewas_220608/attributions/\"\n",
    "        f\"{BASE_DIR}/attributions/\"\n",
    "    )\n",
    "\n",
    "    p = pathlib.Path(base_path).glob(f\"*{endpoint}*{suffix}*\")\n",
    "\n",
    "    files = [x for x in p if x.is_file()]\n",
    "\n",
    "    if not len(files):\n",
    "        continue\n",
    "\n",
    "    images = [PIL.Image.open(x) for x in files]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "    total_width = max(widths)\n",
    "    max_height = sum(heights)\n",
    "\n",
    "    new_im = PIL.Image.new(\"RGB\", (total_width, max_height))\n",
    "\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        new_im.paste(im, (0, x_offset))\n",
    "        x_offset += im.size[1]\n",
    "\n",
    "    new_im.save(base_path / f\"{endpoint_name}{suffix}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03ba41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('react_flask_app')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8a983625ce816da5cd755aefbba2d43d4eb8a4ced825310bb7e5abec74e0b39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
